{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 630667\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://cn004:7077) created by __init__ at /global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/sparkhpc-0.3.post4-py3.7.egg/sparkhpc/sparkjob.py:533 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-db719420e764>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0msj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_to_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_spark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#Register the exit handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/sparkhpc-0.3.post4-py3.7.egg/sparkhpc/sparkjob.py\u001b[0m in \u001b[0;36mstart_spark\u001b[0;34m(self, spark_conf, executor_memory, profiling, graphframes_package, extra_conf)\u001b[0m\n\u001b[1;32m    531\u001b[0m                 \u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/global/software/jupyterhub-spark/spark-2.4.0-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    312\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 314\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=spark://cn004:7077) created by __init__ at /global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/sparkhpc-0.3.post4-py3.7.egg/sparkhpc/sparkjob.py:533 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"1:00\" #1 hour\n",
    "os.environ['SBATCH_PARTITION']='single' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional step if stopwords is not installed on your Spark\n",
    "#import nltk\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process the tweets for relevant/irrelevant classification\n",
    "this is to process the tweets for the naive bayes classification\n",
    "\n",
    "open the tweets, process them, then save them back into the json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files to read and write from\n",
    "in_filename1 = 'LabeledData1.json'\n",
    "in_filename2 = 'LabeledData2.json'\n",
    "in_filename3 = 'LabeledData3.json'\n",
    "out_filename = 'classify.json'\n",
    "\n",
    "# join the 3 labeled data files together\n",
    "rdd1 = sc.textFile(in_filename1)\n",
    "rdd2 = sc.textFile(in_filename1)\n",
    "rdd3 = sc.textFile(in_filename1)\n",
    "joined = rdd1.union(rdd2)\n",
    "joined = joined.union(rdd3)\n",
    "\n",
    "out_handle = open(out_filename, 'w', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the tweet and adds in negation terms\n",
    "# adds a not to every word after a token of logical negation\n",
    "# until the next puncutation mark\n",
    "puncuation = {'\"', ',', '.', '?', '!'}\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/23384351/how-to-add-tags-to-negated-words-in-strings-that-follow-not-no-and-never\n",
    "def check_and_negate(tweet: str):\n",
    "    tweet = tweet.strip()\n",
    "    # check if the end of the tweet has a puncuation mark and add if not\n",
    "    if len(tweet) > 0 and tweet[-1] not in puncuation:\n",
    "        tweet += '.'\n",
    "    tweet = re.sub('n\\'t', ' not', tweet) # replace n't with not\n",
    "    tweet = re.sub('nâ€™t', ' not', tweet)\n",
    "    # add NOT_ to the beginning of each word until a end of sentence mark (.,!?) occurs\n",
    "    tweet = re.sub(r'\\b(?:not|never|no)\\b[\\'\\w\\s]+[.,?!\"]', \n",
    "                   lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1not_\\2', match.group(0)), \n",
    "                   tweet,\n",
    "                   flags=re.IGNORECASE)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# words used in the filter\n",
    "filterWords = {\"tesla\", \"elon\", \"musk\", \"elonmusk\", \"tsla\", \"roadster\", \"supercharger\", \"powerwall\", \"powerpack\", \"modely\",\n",
    "               \"model3\", \"modelx\", \"teslamodely\", \"teslamodels\", \"teslamodel3\", \"teslamodelx\", \"spacex\",\n",
    "               \"teslasuv\", \"teslascience\"}\n",
    "\n",
    "# bigrams used in the filter\n",
    "filterBigrams = {(\"model\", \"y\"), (\"model\", \"s\"), (\"model\", \"3\"), (\"model\", \"x\"), (\"electric\", \"vehicle\"),\n",
    "                 (\"electric\", \"car\"), (\"electric\", \"suv\"), (\"electric\", \"supercar\")}\n",
    "\n",
    "# stop words found using the word cloud (manual extraction)\n",
    "cloud = {'year', 'amp', 'us'}\n",
    "\n",
    "# add filter words to the stop words\n",
    "for word in filterWords:\n",
    "    stop_words.add(word)\n",
    "    \n",
    "# add filter bigrams to the stop words\n",
    "for bi1, bi2 in filterBigrams:\n",
    "    stop_words.add(bi1)\n",
    "    stop_words.add(bi2)\n",
    "    \n",
    "# add cloud to the stop words\n",
    "for word in cloud:\n",
    "    stop_words.add(word)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # remove punctuation and keep only words\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def remove_stop_and_stem(tweet: str):\n",
    "    words = []\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words: # remove stopwords\n",
    "            stemmed = ps.stem(token) # stem the word\n",
    "            if stemmed not in words: # ensure only unique words for binary naive bayes\n",
    "                words.append(ps.stem(token))\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.aclweb.org/anthology/W16-2610\n",
    "\n",
    "# transform emojis into text\n",
    "def transform_emojis(tweet: str):\n",
    "    result = ''\n",
    "    for character in tweet:\n",
    "        temp = character\n",
    "        if temp in emoji.UNICODE_EMOJI:\n",
    "            temp = ' emoji_' + emoji.demojize(temp)[1:-1]\n",
    "        result += temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes the tweet\n",
    "# https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-one-5ac2d06b63fb\n",
    "def process_tweet(tweet: str):\n",
    "    tweet = tweet.lower() # convert to lowercase\n",
    "    tweet = re.sub('#', '', tweet) # remove hashtags\n",
    "    tweet = re.sub('@tesla', 'at_tesla', tweet) # change @Tesla to at_tesla\n",
    "    tweet = re.sub('@elonmusk', 'elonmusk', tweet) # change @elonmusk to at_elonmusk\n",
    "    tweet = re.sub('@\\S+', '', tweet) # remove @username\n",
    "    tweet = re.sub('\\s+', ' ', tweet) # remove multiple whitespace\n",
    "    # https://stackoverflow.com/questions/6038061/regular-expression-to-find-urls-within-a-string\n",
    "    tweet = re.sub('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', tweet) # remove links\n",
    "    tweet = transform_emojis(tweet) # transform emojis\n",
    "    tweet = check_and_negate(tweet)# add negations\n",
    "    tweet = remove_stop_and_stem(tweet) # remove stop words and stem\n",
    "    tweet.strip() # remove excess leading and trailing whitespace\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(tweet: str):\n",
    "    # convert json object into python dict\n",
    "    item = json.loads(tweet)\n",
    "    if 'text' in item: # check that the dictionary contains the key\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_then_process(tweetObject: str):\n",
    "    item = json.loads(tweetObject) # read the tweet object\n",
    "    item['text'] = process_tweet(item['text']) # process the tweet text\n",
    "    return json.dumps(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tweets = joined.filter(lambda x: filter_text(x))\n",
    "\n",
    "processed = text_tweets.map(lambda x: parse_then_process(x))\n",
    "\n",
    "for item in processed.collect():\n",
    "    out_handle.write(item)\n",
    "    out_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file closed\n"
     ]
    }
   ],
   "source": [
    "out_handle.close()\n",
    "print('output file closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
