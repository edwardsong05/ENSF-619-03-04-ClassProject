{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 677832\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"1:00\" #1 hour\n",
    "os.environ['SBATCH_PARTITION']='lattice' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional step if wordnet is not installed on your Spark\n",
    "#import nltk\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment one of the below to get labeled/unlabeled dataset \n",
    "\n",
    "# open files to read and write from\n",
    "in_filename1 = 'LabeledData1.json'\n",
    "in_filename2 = 'LabeledData2.json'\n",
    "in_filename3 = 'LabeledData3.json'\n",
    "out_filename = 'classify_rel_labeled_lemma.json'\n",
    "\n",
    "# join the 3 labeled data files together\n",
    "rdd1 = sc.textFile(in_filename1)\n",
    "rdd2 = sc.textFile(in_filename1)\n",
    "rdd3 = sc.textFile(in_filename1)\n",
    "joined = rdd1.union(rdd2)\n",
    "joined = joined.union(rdd3)\n",
    "\n",
    "\"\"\"\n",
    "in_filename1 = 'allFiltered4.json'\n",
    "joined = sc.textFile(in_filename1)\n",
    "out_filename = 'classify_rel_unlabeled.json'\n",
    "\"\"\"\n",
    "\n",
    "out_handle = open(out_filename, 'w', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the tweet and adds in negation terms\n",
    "# adds a not to every word after a token of logical negation\n",
    "# until the next puncutation mark\n",
    "puncuation = {'\"', ',', '.', '?', '!'}\n",
    "\n",
    "# adapted from https://stackoverflow.com/questions/23384351/how-to-add-tags-to-negated-words-in-strings-that-follow-not-no-and-never\n",
    "def check_and_negate(tweet: str):\n",
    "    tweet = tweet.strip()\n",
    "    # check if the end of the tweet has a puncuation mark and add if not\n",
    "    if len(tweet) > 0 and tweet[-1] not in puncuation:\n",
    "        tweet += '.'\n",
    "    tweet = re.sub('n\\'t', ' not', tweet) # replace n't with not\n",
    "    tweet = re.sub('nâ€™t', ' not', tweet)\n",
    "    # add NOT_ to the beginning of each word until a end of sentence mark (.,!?) occurs\n",
    "    tweet = re.sub(r'\\b(?:not|never|no)\\b[\\'\\w\\s]+[.,?!\"]', \n",
    "                   lambda match: re.sub(r'(\\s+)(\\w+)', r'\\1not_\\2', match.group(0)), \n",
    "                   tweet,\n",
    "                   flags=re.IGNORECASE)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# words used in the filter\n",
    "filterWords = {\"tesla\", \"elon\", \"musk\", \"elonmusk\", \"tsla\", \"roadster\", \"supercharger\", \"powerwall\", \"powerpack\", \"modely\",\n",
    "               \"model3\", \"modelx\", \"teslamodely\", \"teslamodels\", \"teslamodel3\", \"teslamodelx\", \"spacex\",\n",
    "               \"teslasuv\", \"teslascience\"}\n",
    "\n",
    "# bigrams used in the filter\n",
    "filterBigrams = {(\"model\", \"y\"), (\"model\", \"s\"), (\"model\", \"3\"), (\"model\", \"x\"), (\"electric\", \"vehicle\"),\n",
    "                 (\"electric\", \"car\"), (\"electric\", \"suv\"), (\"electric\", \"supercar\")}\n",
    "\n",
    "# stop words found using the word cloud (manual extraction)\n",
    "cloud = {'year', 'amp', 'us', 'one', 'think'}\n",
    "\n",
    "# add filter words to the stop words\n",
    "for word in filterWords:\n",
    "    stop_words.add(word)\n",
    "    \n",
    "# add filter bigrams to the stop words\n",
    "for bi1, bi2 in filterBigrams:\n",
    "    stop_words.add(bi1)\n",
    "    stop_words.add(bi2)\n",
    "    \n",
    "# add cloud to the stop words\n",
    "for word in cloud:\n",
    "    stop_words.add(word)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # remove punctuation and keep only words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stop_and_stem(tweet: str):\n",
    "    words = []\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words: # remove stopwords\n",
    "            stemmed = lemmatizer.lemmatize(token) # stem the word\n",
    "            if stemmed not in words: # ensure only unique words for binary naive bayes\n",
    "                words.append(stemmed)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.aclweb.org/anthology/W16-2610\n",
    "\n",
    "# transform emojis into text\n",
    "def transform_emojis(tweet: str):\n",
    "    result = ''\n",
    "    for character in tweet:\n",
    "        temp = character\n",
    "        if temp in emoji.UNICODE_EMOJI:\n",
    "            temp = ' emoji_' + emoji.demojize(temp)[1:-1]\n",
    "        result += temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes the tweet\n",
    "# https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-one-5ac2d06b63fb\n",
    "def process_tweet(tweet: str):\n",
    "    tweet = tweet.lower() # convert to lowercase\n",
    "    tweet = re.sub('#', '', tweet) # remove hashtags\n",
    "    tweet = re.sub('@tesla', 'at_tesla', tweet) # change @Tesla to at_tesla\n",
    "    tweet = re.sub('@elonmusk', 'elonmusk', tweet) # change @elonmusk to at_elonmusk\n",
    "    tweet = re.sub('@\\S+', '', tweet) # remove @username\n",
    "    tweet = re.sub('\\s+', ' ', tweet) # remove multiple whitespace\n",
    "    # https://stackoverflow.com/questions/6038061/regular-expression-to-find-urls-within-a-string\n",
    "    tweet = re.sub('(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '', tweet) # remove links\n",
    "    tweet = transform_emojis(tweet) # transform emojis\n",
    "    tweet = check_and_negate(tweet)# add negations\n",
    "    tweet = remove_stop_and_stem(tweet) # remove stop words and stem\n",
    "    tweet.strip() # remove excess leading and trailing whitespace\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(tweet: str):\n",
    "    # convert json object into python dict\n",
    "    item = json.loads(tweet)\n",
    "    if 'text' in item: # check that the dictionary contains the key\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_then_process(tweetObject: str):\n",
    "    item = json.loads(tweetObject) # read the tweet object\n",
    "    item['text'] = process_tweet(item['text']) # process the tweet text\n",
    "    return json.dumps(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 50.8 ms, sys: 61 ms, total: 112 ms\n",
      "Wall time: 1.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "text_tweets = joined.filter(lambda x: filter_text(x))\n",
    "\n",
    "processed = text_tweets.map(lambda x: parse_then_process(x))\n",
    "\n",
    "for item in processed.collect():\n",
    "    out_handle.write(item)\n",
    "    out_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output file closed\n"
     ]
    }
   ],
   "source": [
    "out_handle.close()\n",
    "print('output file closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
