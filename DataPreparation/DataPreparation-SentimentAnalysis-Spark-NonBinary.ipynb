{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 679343\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"1:00\" #1 hour\n",
    "os.environ['SBATCH_PARTITION']='lattice' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "sj.wait_to_start()\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process the tweets for sentiment analysis\n",
    "open the tweets, process them, then save them back into the json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files to read and write from\n",
    "in_filename = 'classify_rel_labeled_nonbinary.json'\n",
    "out_filename = 'classify_sen_labeled_nonbinary.json'\n",
    "\n",
    "in_handle = sc.textFile(in_filename)\n",
    "out_handle = open(out_filename, 'w', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = {'elonmusk', 'tesla', 'get', 'at_tesla', 'at_elonmusk'}\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # remove punctuation and keep only words\n",
    "\n",
    "def remove_stop(tweet: str):\n",
    "    words = []\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            words.append(token)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processes the tweet\n",
    "# https://towardsdatascience.com/the-real-world-as-seen-on-twitter-sentiment-analysis-part-one-5ac2d06b63fb\n",
    "def process_tweet(tweet: str):\n",
    "    tweet = tweet.lower() # convert to lowercase\n",
    "    tweet = re.sub('\\s+', ' ', tweet) # remove multiple whitespace\n",
    "    tweet = remove_stop(tweet)\n",
    "    tweet.strip() # remove excess leading and trailing whitespace\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_then_process(tweetObject: str):\n",
    "    item = json.loads(tweetObject) # read the tweet object\n",
    "    item['text'] = process_tweet(item['text']) # process the tweet text\n",
    "    return json.dumps(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_relevant(tweetObject: str):\n",
    "    item = json.loads(tweetObject)\n",
    "    if int(item['isRelevant']) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = in_handle.filter(lambda x: filter_relevant(x))\n",
    "\n",
    "processed = filtered.map(lambda x: parse_then_process(x))\n",
    "\n",
    "for item in processed.collect():\n",
    "    out_handle.write(item)\n",
    "    out_handle.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_handle.close()\n",
    "print('output file closed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
