{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 636255\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Spark Job\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Spark Context\n",
      "RUNNING\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"3:00\" #60 min \n",
    "os.environ['SBATCH_PARTITION']='lattice' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "try:\n",
    "    print('Cleaning up Spark Job')\n",
    "    sj.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sj.wait_to_start()\n",
    "\n",
    "try:\n",
    "    print('Cleaning up Spark Context')\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)\n",
    "print(\"RUNNING\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RELEVANCE PREPROCESSING\n",
    "#remove stop words\n",
    "stop_words = {'elonmusk', 'tesla', 'get', 'at_tesla', 'at_elonmusk'}\n",
    "tokenizer = RegexpTokenizer(r'\\w+') # remove punctuation and keep only words\n",
    "\n",
    "def remove_stop(tweet: str):\n",
    "    words = []\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            words.append(token)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def process_tweet(tweet: str):\n",
    "    tweet = tweet.lower() # convert to lowercase\n",
    "    tweet = re.sub('\\s+', ' ', tweet) # remove multiple whitespace\n",
    "    tweet = remove_stop(tweet)\n",
    "    tweet.strip() # remove excess leading and trailing whitespace\n",
    "    return tweet\n",
    "\n",
    "def parse_then_process(tweetObject: str):\n",
    "    item = json.loads(tweetObject) # read the tweet object\n",
    "    item['text'] = process_tweet(item['text']) # process the tweet text\n",
    "    return json.dumps(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SENTIMENT PREPROCESSING\n",
    "stop_words_sent = {'elonmusk', 'tesla', 'get', 'at_tesla', 'at_elonmusk'}\n",
    "tokenizer_sent = RegexpTokenizer(r'\\w+') # remove punctuation and keep only words\n",
    "\n",
    "def remove_stop_sent(tweet: str):\n",
    "    words = []\n",
    "    tokens = tokenizer_sent.tokenize(tweet)\n",
    "    for token in tokens:\n",
    "        if token not in stop_words_sent:\n",
    "            words.append(token)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def process_tweet_sent(tweet: str):\n",
    "    tweet = tweet.lower() # convert to lowercase\n",
    "    tweet = re.sub('\\s+', ' ', tweet) # remove multiple whitespace\n",
    "    tweet = remove_stop_sent(tweet)\n",
    "    tweet.strip() # remove excess leading and trailing whitespace\n",
    "    return tweet\n",
    "\n",
    "def parse_then_process_sent(tweetObject: str):\n",
    "    item = json.loads(tweetObject) # read the tweet object\n",
    "    item['text'] = process_tweet_sent(item['text']) # process the tweet text\n",
    "    return json.dumps(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in labeled tweets\n",
    "rddLabeled = sc.textFile(\"../Data/CombinedLabeled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  lileeny those are destination chargers and charge most normal cars at 7kw the super charger network c https t co dponggcdnn \n",
      "Relevance Label: 1\n"
     ]
    }
   ],
   "source": [
    "#Extract text and relevance label\n",
    "def getRelLabel(line):\n",
    "    jline = json.loads(line)\n",
    "    return (jline['text'], int(jline['isRelevant']))\n",
    "##Preprocess text for relevance classifier\n",
    "rddRelProcessed = rddLabeled.map(parse_then_process)\n",
    "relLabels = rddRelProcessed.map(getRelLabel)\n",
    "firstLabel = relLabels.first()\n",
    "print(\"Text: \", firstLabel[0], \"\\nRelevance Label:\", firstLabel[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fit relevant classification model\n",
    "pipeRelevant = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('mnb', MultinomialNB(fit_prior=False)),])\n",
    "relData = relLabels.collect()\n",
    "trainText = [i[0] for i in relData]\n",
    "trainLabel = [i[1] for i in relData]\n",
    "modelRelevant = pipeRelevant.fit(trainText, trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  @lileeny @elonmusk Those are destination chargers and charge most “normal cars” at 7KW. The super charger network c… https://t.co/DPoNgGCDnN \n",
      "Sentiment Label: 1\n"
     ]
    }
   ],
   "source": [
    "#Extract text and sentiment label\n",
    "#0 = negative, 1 = neutral, 2 = positive\n",
    "def isRelevant(line):\n",
    "    jline = json.loads(line)\n",
    "    if int(jline['isRelevant']):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def getSentLabel(line):\n",
    "    jline = json.loads(line)\n",
    "    return (jline['text'], int(jline['isRelevant']))\n",
    "\n",
    "##Preprocess text for relevance classifier\n",
    "rddRelevant = rddRelProcessed.filter(isRelevant)\n",
    "rddSentProcessed = rddRelevant.map(parse_then_process_sent)\n",
    "\n",
    "sentLabels = rddLabeled.map(getSentLabel)\n",
    "firstLabel = sentLabels.first()\n",
    "print(\"Text: \", firstLabel[0], \"\\nSentiment Label:\", firstLabel[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Fit sentiment classification model\n",
    "pipeSentiment = Pipeline([('vect', CountVectorizer()),\n",
    "                          ('tfidf', TfidfTransformer()),\n",
    "                          ('mnb', MultinomialNB(fit_prior=False)),])\n",
    "sentData = sentLabels.collect()\n",
    "trainText = [i[0] for i in sentData]\n",
    "trainLabel = [i[1] for i in sentData]\n",
    "modelSentiment = pipeSentiment.fit(trainText, trainLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in full filtered tweet set\n",
    "rddFull = sc.textFile(\"../Data/allFiltered4.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Process and extract text from set\n",
    "def getText(line):\n",
    "    jline = json.loads(line)\n",
    "    return jline['text']\n",
    "rddRelProcessed = rddFull.map(parse_then_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Make predictions on relevant tweets\n",
    "def classifyRelevance(line):\n",
    "    jline = json.loads(line)\n",
    "    label = int(modelRelevant.predict((jline['text'],))[0])\n",
    "    jline['isRelevant'] = label\n",
    "    return json.dumps(jline)\n",
    "\n",
    "rddRelClassified = rddRelProcessed.map(classifyRelevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of relevant tweets:  10206\n"
     ]
    }
   ],
   "source": [
    "#Filter out irrelevant tweets\n",
    "rddRelevant = rddRelClassified.filter(isRelevant)\n",
    "print(\"Number of relevant tweets: \", rddRelevant.count())\n",
    "#Pre process tweets for the classifier\n",
    "rddSentProcessed = rddRelevant.map(parse_then_process_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make sentiment predictions on relevant tweets\n",
    "def classifySentiment(line):\n",
    "    jline = json.loads(line)\n",
    "    label = int(modelSentiment.predict((jline['text'],))[0])\n",
    "    jline['sentiment'] = label\n",
    "    return json.dumps(jline)\n",
    "\n",
    "rddSentClassified = rddSentProcessed.map(classifySentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Take the timestamp and sentiment labels from the rdd\n",
    "def extractSentimentAndTimestamp(line):\n",
    "    jline = json.loads(line)\n",
    "    sentiment = int(jline['sentiment'])\n",
    "    timestamp = int(jline['timestamp_ms'])\n",
    "    return (timestamp, sentiment)\n",
    "\n",
    "timeSent = rddSentClassified.map(extractSentimentAndTimestamp).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1546372267107, 1)\n",
      "(1546372485007, 1)\n",
      "(1546372687470, 1)\n",
      "(1546373494329, 1)\n",
      "(1546373635157, 1)\n",
      "(1546374029442, 1)\n",
      "(1546374526952, 1)\n",
      "(1546374602109, 1)\n",
      "(1546375521745, 1)\n",
      "(1546375739216, 1)\n"
     ]
    }
   ],
   "source": [
    "##Check that output is correct\n",
    "for i in range(10):\n",
    "    print(timeSent[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment        date\n",
      "0          1  2019-01-01\n",
      "1          1  2019-01-01\n",
      "2          1  2019-01-01\n",
      "3          1  2019-01-01\n",
      "4          1  2019-01-01\n",
      "{0, 1}\n"
     ]
    }
   ],
   "source": [
    "##convert timestamp to date\n",
    "def convertMsToDate(timestamp):\n",
    "    return datetime.utcfromtimestamp(int(timestamp)/1000).strftime('%Y-%m-%d')\n",
    "dfSent = pd.DataFrame(timeSent, columns=['timestamp_ms', 'sentiment'])\n",
    "dfSent['date'] = dfSent['timestamp_ms'].apply(convertMsToDate)\n",
    "dfSent = dfSent.drop(['timestamp_ms'], axis = 1)\n",
    "print(dfSent.head())\n",
    "print(set(dfSent['sentiment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-2ce5ea3a3c0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msentSums\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdfDateSent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentSums\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, columns=['Date', 'Sentiment Sum'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#dfDateSent = dfDateSent.sort_values(by=['Date'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/global/software/jupyterhub-spark/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    420\u001b[0m                                          dtype=values.dtype, copy=False)\n\u001b[1;32m    421\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataFrame constructor not properly called!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "sentSums = defaultdict(int)\n",
    "for index, row in dfSent.iterrows():\n",
    "    sentSums[row['date']] += row['sentiment']\n",
    "\n",
    "dfDateSent = pd.DataFrame(sentSums.items())#, columns=['Date', 'Sentiment Sum'])\n",
    "#dfDateSent = dfDateSent.sort_values(by=['Date'])\n",
    "\n",
    "print(dfDateSent.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('2019-01-01', 234), ('2019-01-02', 296), ('2019-01-03', 322), ('2019-01-04', 57), ('2018-12-29', 555), ('2018-12-30', 298), ('2018-12-23', 337), ('2018-12-24', 356), ('2019-01-05', 254), ('2019-01-06', 263), ('2019-01-07', 378), ('2019-01-08', 301), ('2019-01-12', 204), ('2019-01-13', 211), ('2019-01-14', 274), ('2019-01-15', 344), ('2019-01-16', 273), ('2019-01-17', 356), ('2019-01-09', 346), ('2019-01-10', 340), ('2018-12-31', 258), ('2019-01-30', 105), ('2019-01-31', 316), ('2019-02-01', 281), ('2019-02-02', 274), ('2019-02-03', 237), ('2018-12-25', 271), ('2018-12-26', 378), ('2018-12-27', 369), ('2018-12-28', 394), ('2019-01-18', 345), ('2019-01-19', 306), ('2019-01-20', 191), ('2019-01-21', 183), ('2019-01-22', 236), ('2019-01-23', 11)])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentSums.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in stock prices\n",
    "dfStock = pd.read_csv('../Data/tsla_stock.csv')\n",
    "dfStock = dfStock.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis = 1)\n",
    "dfStock = dfStock.rename(index=str, columns={\"Close\": \"Stock Price\"})\n",
    "print(dfStock.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge sentiment dataframe with stock price dataframe\n",
    "dfMerged = dfDateSent.merge(dfStock, how='left', on='Date')\n",
    "dfMerged = dfMerged.set_index('Date')\n",
    "print(dfMerged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "dfMerged.plot(rot=-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
