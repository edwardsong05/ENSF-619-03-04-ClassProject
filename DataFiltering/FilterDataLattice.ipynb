{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sparkhpc.sparkjob:Submitted batch job 609508\n",
      "\n",
      "INFO:sparkhpc.sparkjob:Submitted cluster 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up Spark Job\n",
      "Cleaning up Spark Context\n",
      "Running\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import atexit\n",
    "import sys\n",
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "import findspark\n",
    "from sparkhpc import sparkjob\n",
    "import pandas\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
    "def exitHandler(sj,sc):\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Context')\n",
    "        sc.stop()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        print('Trapped Exit cleaning up Spark Job')\n",
    "        sj.stop()\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "#Parameters for the Spark cluster\n",
    "nodes=3\n",
    "tasks_per_node=8 \n",
    "memory_per_task=1024 #1 gig per process, adjust accordingly\n",
    "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
    "# idle so that others may use the resources.\n",
    "walltime=\"60:00\" #60 min \n",
    "os.environ['SBATCH_PARTITION']='lattice' #Set the appropriate ARC partition\n",
    "\n",
    "sj = sparkjob.sparkjob(\n",
    "     ncores=nodes*tasks_per_node,\n",
    "     cores_per_executor=tasks_per_node,\n",
    "     memory_per_core=memory_per_task,\n",
    "     walltime=walltime\n",
    "    )\n",
    "\n",
    "try:\n",
    "    print('Cleaning up Spark Job')\n",
    "    sj.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sj.wait_to_start()\n",
    "\n",
    "try:\n",
    "    print('Cleaning up Spark Context')\n",
    "    sc.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "sc = sj.start_spark()\n",
    "\n",
    "#Register the exit handler                                                                                                     \n",
    "atexit.register(exitHandler,sj,sc)\n",
    "\n",
    "#You need this line if you want to use SparkSQL\n",
    "sqlCtx=SQLContext(sc)\n",
    "\n",
    "print(\"Running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#If I add the word \"the\" to filterWords, then rdd1 will not be empty and I can write to it\n",
    "filterWords = {\"tesla\", \"elon\", \"musk\", \"elonmusk\", \"tsla\", \"roadster\", \"supercharger\", \"powerwall\", \"powerpack\", \"modely\",\n",
    "               \"model3\", \"modelx\", \"teslamodely\", \"teslamodels\", \"teslamodel3\", \"teslamodelx\", \"spacex\",\n",
    "               \"teslasuv\", \"teslascience\"}\n",
    "filterBigrams = {(\"model\", \"y\"), (\"model\", \"s\"), (\"model\", \"3\"), (\"model\", \"x\"), (\"electric\", \"vehicle\"),\n",
    "                 (\"electric\", \"car\"), (\"electric\", \"suv\"), (\"electric\", \"supercar\")}\n",
    "langs = {\"en\", \"und\"}\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "ps = PorterStemmer()\n",
    "\n",
    "#I am taking the filter words, bigrams, trigrams and putting them in a stemmed set\n",
    "#I will run the tweet through all 6 of these sets and if there are any matches in any of these 4 sets, then it should pass\n",
    "filterWordsStemmed = set()\n",
    "filterBigramsStemmed = set()\n",
    "\n",
    "for word in filterWords:\n",
    "    filterWordsStemmed.add(ps.stem(word))\n",
    "for bigram in filterBigrams:\n",
    "    first, second = bigram\n",
    "    first = ps.stem(first)\n",
    "    second = ps.stem(second)\n",
    "    together = (first, second)\n",
    "    filterBigramsStemmed.add(together)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isRelated(jsonLine):\n",
    "    jDict = json.loads(jsonLine)\n",
    "    if 'id' not in jDict.keys():\n",
    "        return False\n",
    "    if 'user' in jDict.keys() and jDict['user']['screen_name'] == 'elonmusk': #this checks for tweets from @elonmusk regardless of content\n",
    "        return True\n",
    "    if 'lang' in jDict.keys() and jDict['lang'] not in langs: #checks for \"en\" and \"und\" languages\n",
    "        return False\n",
    "    if 'text' in jDict.keys():\n",
    "        words = tokenizer.tokenize(jDict['text'].lower()) #splits up tweet into individual words\n",
    "        if len(words)<3:\n",
    "            return False\n",
    "        for word in words:\n",
    "            if word in filterWords: #checks the filterWords set\n",
    "                return True\n",
    "        bigrams = list(nltk.bigrams(words)) #puts the tweet into bigrams\n",
    "        for bigram in bigrams:\n",
    "            if bigram in filterBigrams: #checks the filterBigrams set\n",
    "                return True\n",
    "\n",
    "        wordstems = list()\n",
    "        for word in words:\n",
    "            wordstems.append(ps.stem(word)) #stems the individual words of the tweet\n",
    "\n",
    "        for word in wordstems:\n",
    "            if word in filterWordsStemmed: #checks the filterWordsStemmed set\n",
    "                return True\n",
    "\n",
    "        bigramstems = list(nltk.bigrams(wordstems)) # puts the stemmed tweet into bigram stems\n",
    "        for bigram in bigramstems:\n",
    "            if bigram in filterBigramsStemmed: # checks the filterBigramStemmed set\n",
    "                return True\n",
    "\n",
    "    return False #returns false if in none of the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "dataPath = './data'\n",
    "dataFiles = [f for f in listdir(dataPath) if isfile(join(dataPath, f))]\n",
    "dataFiles.sort(key=lambda x: int(x.partition(\".\")[0]))\n",
    "print(dataFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "counts = []\n",
    "for fileName in dataFiles:\n",
    "    print(\"Filtering \", fileName)\n",
    "    inFile = sc.textFile('./data/'+fileName)\n",
    "    relatedLines = inFile.filter(lambda x: isRelated(x))\n",
    "    counts.append(relatedLines.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering 1.json\n",
      "Filtering 2.json\n",
      "Filtering 3.json\n",
      "Filtering 4.json\n",
      "Filtering 5.json\n",
      "Filtering 6.json\n",
      "Filtering 7.json\n",
      "Filtering 8.json\n",
      "Filtering 9.json\n",
      "Filtering 10.json\n",
      "Filtering 11.json\n",
      "Filtering 12.json\n",
      "Filtering 13.json\n",
      "Filtering 15.json\n",
      "Filtering 16.json\n",
      "Filtering 17.json\n",
      "Filtering 18.json\n",
      "Filtering 19.json\n",
      "Filtering 20.json\n",
      "Filtering 21.json\n",
      "Filtering 23.json\n",
      "Filtering 25.json\n",
      "Filtering 28.json\n",
      "Done\n",
      "CPU times: user 1.96 s, sys: 884 ms, total: 2.84 s\n",
      "Wall time: 1h 24min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts = []\n",
    "for fileName in dataFiles:\n",
    "    print(\"Filtering\", fileName)\n",
    "    inFile = sc.textFile('./data/'+fileName)\n",
    "    relatedLines = inFile.filter(lambda x: isRelated(x))\n",
    "    outNum = fileName.partition(\".\")[0]\n",
    "    outFileName = './filteredData/' + outNum + 'f.json'\n",
    "    with open(outFileName,\"w\") as f:\n",
    "        for line in relatedLines.collect():\n",
    "            f.write(line+\"\\n\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
